# %% [markdown]
# # Content Based Movie Recommendation System

# %% [markdown]
# ### Import Dependencies

# %%
import pandas as pd
import numpy as np

# %% [markdown]
# ### Load the Datasets

# %%
movies = pd.read_csv('tmdb_5000_movies.csv')
credits = pd.read_csv('tmdb_5000_credits.csv')

# %%
movies.head(1)

# %%
credits.head(1)

# %%
movies.info()

# %%
movies.shape

# %%
credits.shape

# %% [markdown]
# ### Merge movies dataset with credits dataset

# %%
movies = movies.merge(credits, on='title', how='left')
movies.head(1)

# %% [markdown]
# ### Crop the movies dataset

# %%
movies = movies[['movie_id','title','overview', 'genres', 'keywords', 'cast', 'crew']]
movies.head(1)

# %%
# View null values
movies.isnull().sum()

# %%
# Drop null values
movies.dropna(inplace=True)
movies.isnull().sum()

# %%
# Remove duplicates
movies.duplicated().sum()

# %%
### Extract only the names of genres and keywords

# %%
movies['keywords'][0]

# %%
movies['genres'][0]

# %%
import ast
def convert(obj):
    L=[]
    for i in ast.literal_eval(obj):
        L.append(i['name'])
    return L

# %%
movies['keywords'] = movies['keywords'].apply(convert)
movies.head(1)

# %%
movies['genres'] = movies['genres'].apply(convert)
movies.head(1)

# %%
# for cast we take only the first three items from the dictionary
def convert1(text):
    L=[]
    counter = 0
    for i in ast.literal_eval(text):
        if counter<3:
            L.append(i['name'])
            counter +=1
        else:
            break
    return L

# %%
movies['cast'] = movies['cast'].apply(convert1)
movies.head(2)

# %%
def fetch_director(text):
    L=[]
    for i in ast.literal_eval(text):
        if i.get('job') == 'Director': 
            L.append(i.get('name'))
            break
    return L

# %%
movies['crew'][0]

# %%
movies['crew'] = movies['crew'].apply(fetch_director)
movies.head(1)

# %%
#Convert overview column into a list form.
movies['overview']=movies['overview'].apply(lambda x:x.split())

# %%
movies['genres']=movies['genres'].apply(lambda x:[i.replace(" ","") for i in x])

# %%
movies['keywords']=movies['keywords'].apply(lambda x:[i.replace(" ","") for i in x])

# %%
movies['cast']=movies['cast'].apply(lambda x:[i.replace(" ","") for i in x])

# %%
movies['crew']=movies['crew'].apply(lambda x:[i.replace(" ","") for i in x])

# %%
### merging all the columns into one column named tags

# %%
movies['tags'] = movies['overview']+movies['genres']+movies['keywords']+ movies['crew'] + movies['cast']

# %%
new_movies = movies.drop(columns = ['overview','genres','keywords','cast','crew'])

# %%
new_movies

# %%
#feri convert list to string
new_movies['tags'] = new_movies['tags'].apply(lambda x: " ".join(x))
new_movies.head()

# %%
new_movies['tags'][1]

# %%
new_movies.head(5)

# %% [markdown]
# ## Vectorization

# %%
new_movies['tags'] = new_movies['tags'].apply(lambda x: str(x))

# %%
print(movies['tags'].head())
print(type(movies['tags'].iloc[0]))

# %%
# The target is to fetch 6000 most commonly occuring words
# And create a vector for each of the movies in the dataset and find the nearest vector of each movie
# Nearer the vector, More similar are the movies
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features =6000, stop_words='english')
# The value it returns is a scipy sparse matrix so, we must convert this matrix into a numpy array
vectors = cv.fit_transform(new_movies['tags']).toarray()
len(cv.get_feature_names_out())

# %%
vectors.shape

# %%
cv.get_feature_names_out()[1:100]

# %%
# Now we work on finding the similarity
from sklearn.metrics.pairwise import cosine_similarity

# %%
# Here we can see that there are certain words that are mentioned separately, even though they are the same like
# action | actions or actual |actually or dance | dances |dancing
# These kind of words corresponds to the same meaning hence we need to stem these kind of words for our system to perform efficiently
# pip install nltk

# %%
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()

def stem(text):
    y=[]
    for i in text.split():
        y.append(ps.stem(i))
    return " ".join(y)

# %%
new_movies['tags'].apply(stem)

# %%
cv.get_feature_names_out()[0:100]

# %%
# Finding out the similarities between the vectors generated by the CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# %%
similarity= cosine_similarity(vectors)

# %%
similarity

# %%
# As we can see, the first movie has the highest similarity with the first movie (cuz Obviously), and we can see similarities with other movies further 
new_movies[new_movies['title'] == 'Interstellar'].index[0]

# %%
similarity[0]

# %%
sorted(similarity[0])

# %%
# After sorting, we cannot seem to identify 

# %%
enumerate(similarity[0])

# %%
list(enumerate(similarity[0]))

# %%
def recommend(movie):
    index = new_movies[new_movies['title'] == movie].index[0]
    distances = similarity[index]
    movies_list = sorted(list(enumerate(distances)), reverse = True, key = lambda x:x[1])[1:6]

    for i in movies_list:
        print(new_movies.iloc[i[0]].title)
    
    if movie not in new_movies:
        return ["Movie not found. Try another title."]

# %%
recommend('Titanic')

# %%



